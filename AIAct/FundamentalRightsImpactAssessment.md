**Fundamental Rights Impact Assessment (FRIA)

A FRIA assesses how an AI system might affect a person’s fundamental rights - not just privacy, but a full range of rights under the EU Charter (freedom of expression, right to work, non-discrimination, etc.).

Only certain deployers must conduct FRIAs—specifically:
- Public sector bodies
- Private entities providing public services
- Deployers of high-risk systems under Annex III (5)(b) and (c) of the AI Act (education and employment scoring systems)

FRIAs must be completed before a high-risk AI system is put into use by the deployer. The logic is simple: while developers (providers) do their own risk reviews, actual risks often emerge in specific, local use cases—which only deployers can assess.

What’s required?
Article 27 outlines the core elements of a FRIA. Deployers must document:
- The system’s intended use and context
- How long and how often it will be used
- Who is likely to be affected
- What specific risks of harm are involved
- Oversight mechanisms
- What happens if things go wrong (complaint channels, governance)